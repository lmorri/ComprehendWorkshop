{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Text Classification can be used to solve various use-cases like sentiment analysis, spam detection, hashtag prediction etc. This notebook demonstrates the use of Amazon Comprehend to provide text classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting. If you don't specify a bucket, SageMaker SDK will create a default bucket following a pre-defined naming convention in the same region. \n",
    "- The IAM role ARN used to give SageMaker access to your data. It can be fetched using the **get_execution_role** method from sagemaker python SDK. \n",
    "\n",
    "***Note: This role should have AmazonComprehendFullAccess, so it can create and run custom classification jobs***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "import time\n",
    "import pytz\n",
    "import linecache\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "print(role) # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch, Comprehend) on your behalf. Note: This role should have AmazonComprehendFullAccess, so it can create and run custom classification jobs\n",
    "bucket='<<INSERT BUCKET NAME>>' # customize to your bucket, for this workshop the Comprehend policy grants access to buckets with comprehend in the name\n",
    "prefix = 'dbpedia/' #Replace with the prefix under which you want to store the data if needed\n",
    "region = 'us-east-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Now we'll download a dataset from the web on which we want to train the text classification model. BlazingText expects a single preprocessed text file with space separated tokens and each line of the file should contain a single sentence and the corresponding label(s) prefixed by \"\\__label\\__\".\n",
    "\n",
    "In this example, let us train the text classification model on the [DBPedia Ontology Dataset](https://wiki.dbpedia.org/services-resources/dbpedia-data-set-2014#2) as done by [Zhang et al](https://arxiv.org/pdf/1509.01626.pdf). The DBpedia ontology dataset is constructed by picking 14 nonoverlapping classes from DBpedia 2014. It has 560,000 training samples and 70,000 testing samples. The fields we used for this dataset contain title and abstract of each Wikipedia article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/saurabh3949/Text-Classification-Datasets/raw/master/dbpedia_csv.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzvf dbpedia_csv.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect the dataset and the classes to get some understanding about how the data and the label is provided in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head dbpedia_csv/train.csv -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above output, the CSV has 3 fields - Label index, title and abstract. Let us first create a label index to label name mapping and then proceed to preprocess the dataset for ingestion by BlazingText."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will print the labels file (`classes.txt`) to see all possible labels followed by creating an index to label mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat dbpedia_csv/classes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates the mapping from integer indices to class label which will later be used to retrieve the actual class name during inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_label = {} \n",
    "with open(\"dbpedia_csv/classes.txt\") as f:\n",
    "    for i,label in enumerate(f.readlines()):\n",
    "        index_to_label[str(i+1)] = label.strip()\n",
    "print(index_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "We need to preprocess the training data into **space separated tokenized text** format which can be consumed by Amazon Comprehend. Also, as mentioned previously, the class label(s) will be mapped from the classes.txt into the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_instance(row):\n",
    "    cur_row = ''\n",
    "    cur_row = index_to_label[row] \n",
    "    return cur_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transform_instance` will be applied to each data instance in parallel using python's multiprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_file, output_file, testfile=1):\n",
    "    all_rows = ''\n",
    "    with open(input_file, 'r') as csvinfile:\n",
    "        #csv_reader = csv.reader(csvinfile, delimiter='\\n')\n",
    "        count = 0\n",
    "        for row in csvinfile:\n",
    "            if (testfile == 0):\n",
    "                count += 1;\n",
    "                if (count == 200):\n",
    "                    break\n",
    "            category = row.split(',')[0]\n",
    "            title = row.split(',')[1]\n",
    "            document = row.split(title+',')[1]\n",
    "            all_rows += transform_instance(category) + ',' + document\n",
    "    \n",
    "        with open(output_file, 'w') as csvoutfile:\n",
    "            csvoutfile.write(all_rows)\n",
    "            \n",
    "def preprocesstest(input_file, output_file, testfile=1):\n",
    "    all_rows = ''\n",
    "    with open(input_file, 'r') as csvinfile:\n",
    "        #csv_reader = csv.reader(csvinfile, delimiter='\\n')\n",
    "        count = 0\n",
    "        for row in csvinfile:\n",
    "            if (testfile == 0):\n",
    "                count += 1;\n",
    "                if (count == 200):\n",
    "                    break\n",
    "            title = row.split(',')[1]\n",
    "            document = row.split(title+',')[1]\n",
    "            all_rows += document\n",
    "    \n",
    "        with open(output_file, 'w') as csvoutfile:\n",
    "            csvoutfile.write(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Preparing the training dataset\n",
    "\n",
    "preprocess('dbpedia_csv/train.csv', 'dbpedia.train')\n",
    "        \n",
    "# Preparing the test dataset        \n",
    "preprocesstest('dbpedia_csv/test.csv', 'dbpedia.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head dbpedia.train -n 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(channel, file):\n",
    "    s3 = boto3.resource('s3')\n",
    "    data = open(file, \"rb\")\n",
    "    key = channel + '/' + file\n",
    "    s3.Bucket(bucket).put_object(Key=key, Body=data)\n",
    "\n",
    "\n",
    "# caltech-256\n",
    "s3_train_key = \"dbpedia/train\"\n",
    "s3_test_key = \"dbpedia/test\"\n",
    "\n",
    "upload_to_s3(s3_train_key, 'dbpedia.train')\n",
    "upload_to_s3(s3_test_key, 'dbpedia.test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data preprocessing cell might take a minute to run. After the data preprocessing is complete, we need to upload it to S3 so that it can be consumed by SageMaker to execute training jobs. We'll use Python SDK to upload these two files to the bucket and prefix location that we have set above.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to setup an output location at S3, where the model artifact will be dumped. These artifacts are also the output of the algorithm's traning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}output'.format(bucket, prefix)\n",
    "s3_train_location = 's3://{}/{}train'.format(bucket, prefix)+'/'+'dbpedia.train'\n",
    "s3_test_location = 's3://{}/{}test'.format(bucket, prefix)+'/'+'dbpedia.test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Comprehend for custom classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Policy for Comprehend Service role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client(\"iam\")\n",
    "policy_name = \"Comprehendpolicy\"\n",
    "policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"VisualEditor0\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\",\n",
    "                \"s3:DeleteObject\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::*Comprehend*\",\n",
    "                \"arn:aws:s3:::*comprehend*\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "create_policy_response = iam.create_policy(\n",
    "    PolicyName = policy_name,\n",
    "    PolicyDocument = json.dumps(policy_document),\n",
    "    Description='Comprehend Policy'\n",
    ")\n",
    "PolicyArn=create_policy_response[\"Policy\"][\"Arn\"]\n",
    "print(PolicyArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_name = \"ComprehendRole\"\n",
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "          \"Effect\": \"Allow\",\n",
    "          \"Principal\": {\n",
    "            \"Service\": \"comprehend.amazonaws.com\"\n",
    "          },\n",
    "          \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "create_role_response = iam.create_role(\n",
    "    RoleName = role_name,\n",
    "    AssumeRolePolicyDocument = json.dumps(assume_role_policy_document),\n",
    "    Description='Amazon Comprehend service role for classifier.'\n",
    ")\n",
    "\n",
    "iam.attach_role_policy(\n",
    "    RoleName = role_name,\n",
    "    PolicyArn = create_policy_response[\"Policy\"][\"Arn\"]\n",
    ")\n",
    "\n",
    "time.sleep(30) # wait for a minute to allow IAM role policy attachment to propagate\n",
    "\n",
    "role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "print(role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a custom classification training job using the Boto3 SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Boto3 SDK:\n",
    "client = boto3.client('comprehend', region_name=(region))\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "# Create a document classifier\n",
    "create_response = client.create_document_classifier(\n",
    "    InputDataConfig={\n",
    "        'S3Uri': (s3_train_location)\n",
    "    },\n",
    "    DataAccessRoleArn=(role_arn),\n",
    "    DocumentClassifierName='dbpedia-classifier'+(timestamp),\n",
    "    LanguageCode='en'\n",
    ")\n",
    "print(create_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_response = client.describe_document_classifier(\n",
    "    DocumentClassifierArn=create_response['DocumentClassifierArn'])\n",
    "    status = describe_response[\"DocumentClassifierProperties\"][\"Status\"]\n",
    "    now = datetime.now(pytz.utc)\n",
    "    elapsed = now - describe_response[\"DocumentClassifierProperties\"][\"SubmitTime\"]\n",
    "    print(\"DocumentClassifierProperties: {}   (elapsed = {})\".format(status, elapsed))\n",
    "    \n",
    "    if status == \"TRAINED\" or status == \"IN_ERROR\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(15)\n",
    "    \n",
    "documentclassifier = describe_response[\"DocumentClassifierProperties\"][\"DocumentClassifierArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve metrics of the Comprehend custom classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics=describe_response[\"DocumentClassifierProperties\"][\"ClassifierMetadata\"]\n",
    "print(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Inference\n",
    "Once the training is done, we can create a job to classify documents with the Amazon Comprehend custom classifier. We will run this against our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_response = client.start_document_classification_job(\n",
    "    InputDataConfig={\n",
    "        'S3Uri': (s3_test_location),\n",
    "        'InputFormat': 'ONE_DOC_PER_LINE'\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        'S3Uri': (s3_output_location)\n",
    "    },\n",
    "    DataAccessRoleArn=(role_arn),\n",
    "    DocumentClassifierArn=(documentclassifier),\n",
    "    JobName='dbpedia-classification Job'+(timestamp)\n",
    ")\n",
    "\n",
    "print(\"Start response: %s\\n\", start_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_response = client.describe_document_classification_job(JobId=start_response['JobId'])\n",
    "    status = describe_response[\"DocumentClassificationJobProperties\"][\"JobStatus\"]\n",
    "    now = datetime.now(pytz.utc)\n",
    "    elapsed = now - describe_response[\"DocumentClassificationJobProperties\"][\"SubmitTime\"]\n",
    "    print(\"DocumentClassificationJobProperties: {}   (elapsed = {})\".format(status, elapsed))\n",
    "    \n",
    "    if status == \"COMPLETED\" or status == \"FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(15)\n",
    "output_location = describe_response[\"DocumentClassificationJobProperties\"][\"OutputDataConfig\"]\n",
    "outputs3=output_location[\"S3Uri\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the classification job has run lets download and view the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3copylocation = (outputs3)\n",
    "!aws s3 cp $s3copylocation .\n",
    "!tar -xzvf output.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will pick a random entry from the dbpedia.test data and its corresponding result from the Amazon Comprehend analysis job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines = sum(1 for line in open('predictions.jsonl'))\n",
    "random_result = random.randint(1,(num_lines))\n",
    "\n",
    "predictionresult = json.loads(linecache.getline('predictions.jsonl', random_result))\n",
    "testdata = linecache.getline('dbpedia.test', random_result)\n",
    "\n",
    "print(predictionresult[\"Classes\"])\n",
    "print(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
